{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11475965,"sourceType":"datasetVersion","datasetId":7192357}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 📦 Clean up conflicting packages from Kaggle's base environment\n# These packages either conflict with our dependencies (e.g., Google Generative AI, LangGraph),\n# or are not needed for this project (e.g., kfp, spacy, fastai, ydata-profiling).\n!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n\n# 🚀 Install specific versions of LangGraph and related packages used in this notebook\n# - langgraph: the core library for agent graphs and orchestration\n# - langgraph-prebuilt: prebuilt components like ToolNode\n# - langchain-google-genai: LangChain integration with Gemini models\n!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ⚠️ THIS CELL MAY SHOW SOME DEPENDENCIES ERRORS, PLEASE IGNORE AS IT DOES NOT IMPACT THE REST OF THE NOTEBOOK\n# 📚 Install additional packages used for RAG and document processing\n\n# chromadb: lightweight vector database for storing and querying embeddings\n!pip install -qU \"chromadb==0.6.3\"\n\n# tqdm: for progress bars during indexing or loops (used in embedding steps)\n!pip install tqdm\n\n# pymupdf: for parsing and extracting text + metadata (like page numbers) from PDF documents\n!pip install pymupdf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔐 Load and set the Google API key from Kaggle secrets\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n# Retrieve the API key securely from Kaggle's secret manager\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\n# Set the key as an environment variable so it can be picked up by client libraries like Gemini or LangChain\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG","metadata":{}},{"cell_type":"markdown","source":"🔍 **Why and How RAG is Used in This Project**\n\nIn this project, Retrieval-Augmented Generation (RAG) is used to provide accurate, grounded answers about the ElevatePro community — including its mission, values, events, and internal guidelines. Rather than relying solely on the LLM’s general knowledge, RAG allows the assistant to retrieve relevant context from official community documents stored in a **ChromaDB** vector database. To optimize retrieval quality, three different chunking strategies were tested: **fixed-size chunking**, **one chunk per document**, and **semantic chunking** using the unstructured package. This enabled experimentation with how different document structures affect answer accuracy. When a user asks a question, the assistant queries the indexed chunks to find the most relevant passages and uses them to craft informed, source-backed responses.","metadata":{}},{"cell_type":"markdown","source":"## Indexing","metadata":{}},{"cell_type":"markdown","source":"🧠 **Indexing Phase of RAG**\n\nThe indexing phase is a crucial step in the RAG workflow where source documents are preprocessed, chunked, and converted into numerical vector representations (embeddings). These embeddings capture the semantic meaning of each chunk and are stored in a ChromaDB collection for fast similarity-based retrieval. Depending on the chunking strategy used (fixed-size, full-doc, or semantic), the granularity of stored knowledge varies. This phase ensures that when a user asks a question, the system can quickly retrieve the most relevant pieces of context to support an accurate and grounded response.","metadata":{}},{"cell_type":"code","source":"# ⚠️ This might show dependency warnings/errors — they can be safely ignored.\n# 💡 Uncomment the line below if you want to experiment with semantic chunking using the `unstructured` library.\n# It enables automatic extraction of logical sections from PDF files (e.g., headings, paragraphs, etc.)\n\n# !pip install \"unstructured[pdf]\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # PyMuPDF for PDF reading\nimport uuid  # For generating unique IDs for chunks\nimport re    # For regex-based emoji removal\nfrom IPython.display import display, Markdown, clear_output\n# from unstructured.partition.pdf import partition_pdf  # Optional semantic chunking\n\n# 🧼 Utility to remove emojis from text to keep embeddings clean and consistent\ndef remove_emojis(text):\n    \"\"\"\n    Function: remove_emojis\n    Description: Removes emojis and special Unicode symbols from the input text.\n                 Useful to clean content before generating embeddings.\n    \"\"\"\n    emoji_pattern = re.compile(\n        \"[\"\n\n        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n        \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n        \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n        \"\\U0001F1E0-\\U0001F1FF\"  # Flags\n        \"\\U00002700-\\U000027BF\"  # Dingbats\n        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n        \"🤝\" \"🧠\" \"🟦\" \"🟪\" \"🧭\"   # Specific additional emojis\n        \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n# 📏 Utility to split long text into smaller overlapping chunks for embedding\ndef split_text_into_chunks(text, chunk_size=500, overlap=50):\n    \"\"\"\n    Function: split_text_into_chunks\n    Description: Splits input text into overlapping chunks of approximately `chunk_size` characters,\n                 with `overlap` characters shared between chunks to preserve context continuity.\n    \"\"\"\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_length = 0\n\n    for word in words:\n        word_len = len(word) + 1  # Include space\n        if current_length + word_len > chunk_size:\n            chunks.append(\" \".join(current_chunk))\n            # Add overlap: keep the last N words from previous chunk\n            overlap_words = current_chunk[-(overlap // 5):]  # Rough estimate: 5 chars per word\n            current_chunk = overlap_words + [word]\n            current_length = sum(len(w)+1 for w in current_chunk)\n        else:\n            current_chunk.append(word)\n            current_length += word_len\n\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk))\n\n    return chunks\n\n\n# 📄 Extracts chunks from a PDF file, either one big chunk per doc or multiple per page\ndef extract_chunks_from_pdf(filepath, chunk_size=500, overlap=50, one_chunk_per_doc=False):\n    \"\"\"\n    Function: extract_chunks_from_pdf\n    Description: Parses a PDF and returns a list of chunks with associated metadata.\n                 Supports two modes:\n                 - one_chunk_per_doc: treat the entire PDF as a single chunk.\n                 - per-page chunking with optional overlap.\n    \"\"\"\n    doc = fitz.open(filepath)\n    filename = filepath.split('/')[-1]\n    chunks = []\n\n    if one_chunk_per_doc:\n        # Combine all pages into one large cleaned chunk\n        full_text = \"\"\n        for page in doc:\n            full_text += page.get_text() + \"\\n\"\n        full_text = remove_emojis(full_text)\n        chunks.append({\n            \"id\": str(uuid.uuid4()),\n            \"text\": full_text.strip(),\n            \"metadata\": {\n                \"filename\": filename,\n                \"page_number\": -1,  # Indicates full-document chunk\n            }\n        })\n    else:\n        # Chunk per page with optional overlap\n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            text = page.get_text()\n            clean_text = remove_emojis(text)\n            page_chunks = split_text_into_chunks(clean_text, chunk_size, overlap)\n\n            for chunk in page_chunks:\n                chunks.append({\n                    \"id\": str(uuid.uuid4()),\n                    \"text\": chunk,\n                    \"metadata\": {\n                        \"filename\": filename,\n                        \"page_number\": page_num + 1,\n                    }\n                })\n\n    return chunks\n\n\n# 🧠 Semantic chunking using the `unstructured` library (optional, more intelligent splitting)\ndef extract_unstructured_chunks(pdf_path: str, source_name: str = None):\n    \"\"\"\n    Function: extract_unstructured_chunks\n    Description: Uses the `unstructured` library to parse PDF documents into semantically meaningful\n                 sections (titles, narrative text, tables, etc.), capturing rich metadata like page number and category.\n    \"\"\"\n    elements = partition_pdf(filename=pdf_path)  # Requires unstructured[pdf]\n    chunks = []\n\n    for i, el in enumerate(elements):\n        if el.text.strip():  # Only include non-empty sections\n            chunk = {\n                \"id\": str(uuid.uuid4()),\n                \"text\": el.text.strip(),\n                \"metadata\": {\n                    \"type\": el.category,  # E.g., Title, NarrativeText, ListItem\n                    \"filename\": source_name or pdf_path.split(\"/\")[-1],\n                    \"element_index\": i,\n                    'page_number': el.metadata.page_number or -1\n                }\n            }\n            chunks.append(chunk)\n\n    return chunks\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Document Chunking","metadata":{}},{"cell_type":"code","source":"# 📌 Choose your document chunking strategy for indexing\n# Options:\n# - \"semantic_chunking\": uses the `unstructured` library to detect logical sections\n# - \"one_chunk_per_doc\": treats the entire PDF as a single chunk (simpler, fewer embeddings)\n# - \"fixed_chunking\": splits text into fixed-size chunks with overlap (preserves context flow)\n\nchuncking_startegy = \"one_chunk_per_doc\"  # 👈 current strategy in use\n\n# ✅ Confirm strategy choice\nprint(f\"📂 Chunking strategy selected: {chuncking_startegy}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# 📥 Download the community document dataset from Kaggle Hub\n# This retrieves a dataset named 'community-docs' from the user's Kaggle account\n\ncommunity_docs_path = kagglehub.dataset_download('riadbensalem/community-docs')\nprint(community_docs_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\nchunks = []\n\n# 🔍 Get list of all PDF files in the /kaggle/input/community-docs directory\npdf_dir = Path(community_docs_path)  # Ensure it's a Path, not string\npathlist = list(pdf_dir.glob(\"*.pdf\"))\n\n# 🚨 Safety check: make sure there are PDFs to process\nif not list(pathlist):\n    raise FileNotFoundError(\"❌ No PDF files found in /kaggle/input/community-docs. Please upload at least one.\")\n\n# 📦 Loop through each PDF and apply the selected chunking strategy\nfor path in pathlist:\n    print(\"📄 Chunking File:\", str(path).split('/')[-1])\n\n    if chuncking_startegy == \"one_chunk_per_doc\":\n        one_chunk_per_doc = True\n        # Entire PDF is treated as a single chunk\n        chunks.append(extract_chunks_from_pdf(str(path), one_chunk_per_doc=one_chunk_per_doc))\n\n    if chuncking_startegy == \"fixed_chunking\":\n        one_chunk_per_doc = False\n        # Chunk per page with overlap\n        chunks.append(extract_chunks_from_pdf(str(path), one_chunk_per_doc=one_chunk_per_doc))\n\n    if chuncking_startegy == \"semantic_chunking\":\n        # Use unstructured to extract semantic elements (title, paragraphs, etc.)\n        chunks.append(extract_unstructured_chunks(str(path), str(path).split('/')[-1]))\n\n# 🔄 Flatten the nested list of chunks into a single list\nchunks_flatten = [chunk for pdf in chunks for chunk in pdf]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Embedding Vectors Computation","metadata":{}},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    \"\"\"\n    Class: GeminiEmbeddingFunction\n    Description:\n        Custom embedding function to integrate Gemini embeddings into ChromaDB.\n        Allows toggling between document embedding and query embedding modes.\n    \"\"\"\n    document_mode = True  # Toggle for document vs. query embedding\n\n    def __init__(self):\n        \"\"\"\n        Function: __init__\n        Description:\n            Initializes the LangChain embedding wrapper for Gemini's text-embedding-004 model.\n        \"\"\"\n        self.embedder = GoogleGenerativeAIEmbeddings(\n            model=\"models/text-embedding-004\"\n        )\n\n    def __call__(self, input: Documents) -> Embeddings:\n        \"\"\"\n        Function: __call__\n        Description:\n            Generates embeddings from a list of input documents or queries.\n            If document_mode is True, calls embed_documents().\n            If document_mode is False, calls embed_query() for each item individually.\n        \"\"\"\n        if self.document_mode:\n            return self.embedder.embed_documents(input)  # Batch embed documents\n        else:\n            return [self.embedder.embed_query(q) for q in input]  # Embed queries one-by-one\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import chromadb\nfrom tqdm import tqdm\n\n# 🔌 Initialize the Gemini embedding function for ChromaDB\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True  # Ensures we're embedding documents, not queries\n\n# ⚙️ Create a ChromaDB client and choose collection name based on selected strategy\nchroma_client = chromadb.Client()\n\nif chuncking_startegy == \"fixed_chunking\":\n    # For fixed-size overlapping chunks\n    collection = chroma_client.get_or_create_collection(\"rag_chunks\", embedding_function=embed_fn)\n\nif chuncking_startegy == \"one_chunk_per_doc\":\n    # For full-document chunks\n    collection = chroma_client.get_or_create_collection(\"rag_docs\", embedding_function=embed_fn)\n\nif chuncking_startegy == \"semantic_chunking\":\n    # For semantically chunked content\n    collection = chroma_client.get_or_create_collection(\"rag_chunks_semantic\", embedding_function=embed_fn)\n\n\n# 📥 Batching function to index chunks into ChromaDB in batches (avoids rate limits)\ndef batch_add_to_chromadb(collection, chunks, batch_size=100):\n    \"\"\"\n    Function: batch_add_to_chromadb\n    Description:\n        Adds chunks to a ChromaDB collection in batches. Each chunk includes an ID, the text content,\n        and associated metadata. This avoids hitting embedding API limits and speeds up bulk indexing.\n\n    Args:\n        collection: The ChromaDB collection to add documents to.\n        chunks (list): A list of chunk dictionaries with 'id', 'text', and 'metadata'.\n        batch_size (int): Number of chunks to process per batch (default is 100).\n    \"\"\"\n    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Indexing into ChromaDB\"):\n        batch = chunks[i:i + batch_size]\n        collection.add(\n            documents=[c[\"text\"] for c in batch],\n            metadatas=[c[\"metadata\"] for c in batch],\n            ids=[c[\"id\"] for c in batch],\n        )\n\n# 🚀 Start indexing all extracted and flattened chunks into ChromaDB\nbatch_add_to_chromadb(collection, chunks_flatten)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Retrieval Test","metadata":{}},{"cell_type":"markdown","source":"📥 **Retrieval Phase of RAG**\n\nIn the retrieval phase of the RAG workflow, user queries are transformed into vector embeddings and compared against pre-indexed document chunks stored in ChromaDB. This semantic search process identifies the most relevant pieces of content based on meaning, not just keyword matches. The top results — typically the closest chunks in vector space — are then returned along with their metadata (such as source file and confidence score). These retrieved chunks form the external knowledge grounding the final answer generated by the LLM.","metadata":{}},{"cell_type":"code","source":"# 🔄 Switch the embedding function to query mode\n# This ensures embeddings are generated using the correct task type (retrieval_query)\nembed_fn.document_mode = False\n\n# 🔍 Define a sample user query to test semantic retrieval from the vector database\nquery = \"what is the mission of the community?\"\n\n# 🔎 Perform semantic search in the ChromaDB collection\n# It returns the top 2 most relevant chunks based on cosine similarity\nresult_docs = collection.query(query_texts=[query], n_results=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(collection.count())\nprint(result_docs[\"documents\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AI Agent","metadata":{}},{"cell_type":"markdown","source":"### 🤖 AI Agent in This Project\n\nIn this project, an AI agent is built using the **LangGraph** framework, which enables the orchestration of an intelligent, tool-using assistant. The agent is designed to act as an onboarding and support assistant for a professional community. It can engage in natural conversations, respond to user questions, and dynamically decide when to call tools based on intent.\n\nThe agent leverages two main tools:\n1. **`get_info`**: A Retrieval-Augmented Generation (RAG) tool that queries a vector database to answer questions about the community (e.g., its mission, values, guidelines).\n2. **`score_application`**: A tool that uses the LLM to evaluate membership applications by scoring responses based on predefined criteria.\n3. **`check_new_applications`**: A utility tool that connects to a Google Sheet where application forms are submitted. It detects and evaluates only **new applications** (avoiding duplicates), then displays a color-coded summary of the results in a Markdown table and exports the data to a downloadable CSV file.\n\nUsing **LangGraph's node-based architecture**, the agent handles message history, routes logic conditionally (e.g., chat → tool → response), and maintains state across interactions. It allows both user-driven and model-driven tool invocation, making it capable of reasoning, calling functions, and continuing conversation — all while grounded in real data.\n\nThis makes the AI agent not just a chatbot, but a structured decision-making assistant that can automate onboarding tasks and support community growth intelligently.\n","metadata":{}},{"cell_type":"code","source":"from google.genai import types\nimport typing_extensions as typing\nimport enum\nfrom pprint import pprint\nimport json\nimport re\n\nclass Verdict(enum.Enum):\n    \"\"\"\n    Class: Verdict\n    Description:\n        Enumeration of possible outcomes when evaluating a community membership application.\n        The values are:\n        - APPROVE: The applicant is a strong fit and should be accepted.\n        - REVIEW: The applicant might be a fit, but requires human review.\n        - REJECT: The applicant is not a fit for the community.\n    \"\"\"\n    APPROVE = \"approve\"\n    REVIEW = \"review\"\n    REJECT = \"reject\"\n\n\nclass Score(typing.TypedDict):\n    \"\"\"\n    Class: Score\n    Description:\n        Defines the expected structured output of the LLM when evaluating a community application.\n        It ensures consistency in how application scoring is handled and interpreted.\n\n        This schema is used in the `score_application` tool and helps:\n        - Enforce a structured format for LLM output\n        - Facilitate parsing and validation\n        - Guide downstream logic and display (e.g., storing scores, showing feedback)\n\n    Fields:\n        - score (int): A numerical score from 1 to 10 based on evaluation criteria.\n        - verdict (Verdict): A high-level recommendation (approve, review, reject).\n        - reasoning (str): A human-readable explanation of why the given score was assigned.\n    \"\"\"\n    score: int\n    verdict: Verdict\n    reasoning: str\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🧠 **Prompt Engineering Techniques Used**\n\nThis project combines multiple advanced prompt engineering techniques to guide the behavior of the AI agent. First, **role-based prompting** is used to define the assistant’s identity and goals — as either a community onboarding guide or an application reviewer. To ensure the agent uses external tools effectively, **tool-use prompting** is applied, with clearly labeled tool names, input formats, and specific examples of when to use each tool.\n\nAdditionally, **few-shot prompting** is incorporated via examples of both user inputs and assistant responses, helping the model learn how to respond in tool-call format through imitation. The `get_info` tool is guided using a structured **chain-of-thought (CoT)** strategy: the system prompt instructs the agent to break down the user query into topics, extract relevant keywords, and form a retrieval query — encouraging step-by-step reasoning before action.\n\nFinally, the `score_application` tool uses **structured output prompting**, specifying a clear JSON schema and scoring rubric to guide the LLM’s response format. This mix of techniques ensures the agent behaves reliably, reasons clearly, and integrates seamlessly with tools in a LangGraph-driven environment.","metadata":{}},{"cell_type":"code","source":"from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages import SystemMessage\n\n# 🧠 LangGraph State Definition for Agent\nclass ReviewState(TypedDict):\n    \"\"\"\n    Class: ReviewState\n    Description:\n        Defines the structure of the shared state used by the LangGraph agent.\n\n        This state allows different nodes (chat, scoring, tools) to share information and update progress\n        during the onboarding or application review process.\n\n    Fields:\n        - messages: A running history of messages exchanged. Annotated with `add_messages` to allow\n                    LangGraph to append messages incrementally between nodes.\n        - applications: A list of raw application texts submitted for scoring.\n        - scores: A list of structured score objects (`Score`), each produced by the `score_application` tool.\n        - conversation_done: A boolean flag indicating whether the user has chosen to end the conversation.\n    \"\"\"\n    messages: Annotated[list, add_messages]\n    applications: list[str]\n    scores: list[Score]\n    conversation_done: bool\n\n\n# 🤖 System Prompt for Onboarding Assistant Agent (used by chatbot node)\nAIONBOARD_SYSINT_backup1 = SystemMessage(content=\"You are an AI Onboarding Assistant for our organization's community.  Your mission is to help and support new members by:\\n - Explaining the community’s mission, vision, and core values.\\n - Sharing how members can get involved in initiatives, join groups, or contribute to projects.\\n - Helping them discover relevant resources, guidelines, and communication channels.\\n - Encouraging them to actively engage and contribute to the community.\\n\\n You have access to three tools and should use them whenever relevant, instead of guessing:\\n 1. get_info(query: str): Retrieves authoritative information from community documentation using RAG.  Use this when users ask about the community's goals, onboarding process, contribution opportunities, resources, or policies. This must be your exclusive source of information for these topics or any question related the organization and community.\\n When you receive questions about the community or need to provide guidance, follow this reasoning process to decide if and how to use the get_info tool:\\n - Step 1: Identify the general topic(s) involved in the user query — e.g. mission, onboarding, contribution, policies, communication platforms, etc.\\n - Step 2: Break down the query into specific sub-questions or keywords that best capture the user's intent.\\n - Step 3: Rephrase or extract a clean query from the conversation that is optimal for retrieving information.\\n - Step 4: Call the get_info tool with that query. Wait for the result, then synthesize a helpful and accurate response using what was retrieved.\\n\\n 2. score_application: Use the tool score_application to evaluate if a new applicant is a good fit for the community.  The input should include their full application context (background, motivation, interests, etc.).\\n\\n Be warm, welcoming, and helpful. Avoid guessing — use the tools.\\n \\n\\n Examples:\\n User: What are the core goals of this community?\\n Assistant: <tool_call>\\nget_info(\\\"core goals of the community\\\")\\n</tool_call>\\n\\n User: Here's a new application: \\\"I'm a software engineer passionate about mentoring others and contributing to AI-for-social-impact projects.\\\"\\n Assistant: <tool_call>\\nscore_application(\\\"I'm a software engineer passionate about mentoring others and contributing to AI-for-social-impact projects.\\\")\\n</tool_call>.\" )\n\nAIONBOARD_SYSINT_backup2 = SystemMessage(content=\"You are an AI Onboarding Assistant for our organization's community.  Your mission is to help and support new members by:\\n - Explaining the community’s mission, vision, and core values.\\n - Sharing how members can get involved in initiatives, join groups, or contribute to projects.\\n - Helping them discover relevant resources, guidelines, and communication channels.\\n - Encouraging them to actively engage and contribute to the community.\\n\\n You have access to three tools and should use them whenever relevant, instead of guessing:\\n 1. get_info(query: str): Retrieves authoritative information from community documentation using RAG.  Use this when users ask about the community's goals, onboarding process, contribution opportunities, resources, or policies. This must be your exclusive source of information for these topics or any question related the organization and community.\\n When you receive questions about the community or need to provide guidance, follow this reasoning process to decide if and how to use the get_info tool:\\n - Step 1: Identify the general topic(s) involved in the user query — e.g. mission, onboarding, contribution, policies, communication platforms, etc.\\n - Step 2: Break down the query into specific sub-questions or keywords that best capture the user's intent.\\n - Step 3: Rephrase or extract a clean query from the conversation that is optimal for retrieving information.\\n - Step 4: Call the get_info tool with that query. Wait for the result, then synthesize a helpful and accurate response using what was retrieved.\\n\\n 2. score_application: Use the tool score_application to evaluate if a new applicant is a good fit for the community.  The input should include their full application context (background, motivation, interests, etc.).\\n\\n Be warm, welcoming, and helpful. Avoid guessing — use the tools.\\n 3. check_new_applications: Use this tool when the user asks you to review new applications submitted through the form.You MUST call the tool using this syntax: <tool_call>check_new_applications()</tool_call>❗ Do NOT write Python code or use print() or default_api. Never say: `print(check_new_applications())`. This tool fetches new responses from a Google Sheet, scores them, prints results in a table, and saves to a CSV. \\n\\n Examples:\\n User: What are the core goals of this community?\\n Assistant: <tool_call>\\nget_info(\\\"core goals of the community\\\")\\n</tool_call>\\n\\n User: Here's a new application: \\\"I'm a software engineer passionate about mentoring others and contributing to AI-for-social-impact projects.\\\"\\n Assistant: <tool_call>\\nscore_application(\\\"I'm a software engineer passionate about mentoring others and contributing to AI-for-social-impact projects.\\\")\\n</tool_call>. \\n User: Can you check for new applications and score them? Assistant: <tool_call>check_new_applications()</tool_call>\" )\n\nAIONBOARD_SYSINT = SystemMessage(content=\"You are an AI Onboarding Assistant for our organization's community.  Your mission is to help and support new members by:\\n - Explaining the community’s mission, vision, and core values.\\n - Sharing how members can get involved in initiatives, join groups, or contribute to projects.\\n - Helping them discover relevant resources, guidelines, and communication channels.\\n - Encouraging them to actively engage and contribute to the community.\\n\\n You have access to three tools and should use them whenever relevant, instead of guessing:\\n 1. get_info(query: str): Retrieves authoritative information from community documentation using RAG.  Use this when users ask about the community's goals, onboarding process, contribution opportunities, resources, or policies. This must be your exclusive source of information for these topics or any question related the organization and community.\\n When you receive questions about the community or need to provide guidance, follow this reasoning process to decide if and how to use the get_info tool:\\n - Step 1: Identify the general topic(s) involved in the user query — e.g. mission, onboarding, contribution, policies, communication platforms, etc.\\n - Step 2: Break down the query into specific sub-questions or keywords that best capture the user's intent.\\n - Step 3: Rephrase or extract a clean query from the conversation that is optimal for retrieving information.\\n - Step 4: Call the get_info tool with that query. Wait for the result, then synthesize a helpful and accurate response using what was retrieved.\\n\\n 2. score_application: Use the tool score_application to evaluate if a new applicant is a good fit for the community.  The input should include their full application context (background, motivation, interests, etc.).\\n\\n Be warm, welcoming, and helpful. Avoid guessing — use the tools.\\n 3. check_new_applications: Use this tool when the user asks you to review new applications submitted through the form.You MUST call the tool using this syntax: <tool_call>check_new_applications()</tool_call>❗ Do NOT write Python code or use print() or default_api. Never say: `print(check_new_applications())`. This tool fetches new responses from a Google Sheet, scores them, prints results in a table, and saves to a CSV. \\n\\n\" )\n\n# 🧠 System Prompt for Application Scoring Agent (used by scoring node)\nAISCORER_SYSINT = SystemMessage(content=\"You are an assistant evaluating applications to join a professional community.  Please read the applicant’s responses and assign a score from 1 to 10 based on the following criteria:  Scoring Criteria:   1. Motivation & Intent (max 4 points):     - Clear and thoughtful reason for joining.     - Alignment with the community’s purpose: collaboration, professional growth, impact     - Strong motivation to learn or contribute.  \\n\\n  2. Background & Relevance (max 3 points):   - Professional background relevant to the community (e.g., tech, design, entrepreneurship, social impact).     - Experience that could add value to the group.  \\n\\n  3. Contribution Potential (max 3 points)     - Willingness and clarity in how they want to contribute (e.g., speaking, organizing, mentoring, sharing resources).     - Realistic and actionable contribution ideas.  \\n\\n  Acceptance Threshold:  - 8-10: Strong fit – recommend approval  - 5-7: Medium fit – recommend review by a human.  - 1-4: Weak fit – recommend rejection or feedback.eak fit – recommend rejection or feedback.  \\n\\n  Return the result ina valid json format with the fields:   score: X   verdict: Enum of 'approve' or 'review' or 'reject'   Reasoning: string where you explain the scores you gave under each Scoring Criteria\")\n\n\n\n# 📝 Legacy versions of the system prompts (not actively used)\nAIONBOARD_SYSINT_old = (\"system\", \"...\")  # Legacy text version of onboarding instruction\nAISCORER_SYSINT_old = (\"system\", \"...\")   # Legacy text version of scorer instruction\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  LLM Initialization and Tool Registration  ","metadata":{}},{"cell_type":"markdown","source":"In this section, we define and register the tools available to the AI agent. These tools include:\n\n- `get_info`: A RAG-based tool that retrieves authoritative information from the community’s knowledge base.\n- `score_application`: A scoring tool that evaluates user-submitted applications using a structured rubric.\n\nWe also initialize two Gemini models:\n\n- `llm_with_tools`: The main conversational model, configured to understand and invoke tools when needed.\n- `llm_scorer`: A specialized model that returns structured JSON output for application evaluation.\n\nFinally, we register the tools using LangGraph’s `ToolNode` and bind them to the appropriate LLM. This enables dynamic, context-aware tool calling during conversation, making the agent both interactive and intelligent.\n\nutilities:\n- **`parse_score_response`** (utility): Validates and parses the raw response from the LLM scorer into a clean, structured format (`Score`), including type checking and error handling.\n- **`google_sheet_to_dataframe_api_key`** (utility): Reads a Google Spreadsheet using an API key and converts it to a pandas DataFrame.","metadata":{}},{"cell_type":"code","source":"GOOGLE_SHEET_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_SHEET_API_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_score_response(response: str) -> Score:\n    \"\"\"\n    Function: parse_score_response\n    Description:\n        Parses a raw string response from the LLM into a structured `Score` TypedDict.\n\n        It removes markdown formatting (e.g., ```json) if present, extracts the JSON payload,\n        validates that all expected fields are present, and converts the `verdict` string to an Enum.\n\n    Raises:\n        ValueError if the response is not valid JSON or does not match the expected schema.\n\n    Returns:\n        Score: A structured object containing the score, verdict, and reasoning.\n    \"\"\"\n    try:\n        if \"```json\" in response:\n            response = re.search(r\"```json\\n(.*?)\\n```\", response, re.DOTALL).group(1)\n\n        data = json.loads(response)\n\n        if not all(key in data for key in (\"score\", \"verdict\", \"Reasoning\")):\n            raise ValueError(\"Missing one or more required keys in response.\")\n\n        score = int(data[\"score\"])\n        verdict_str = data[\"verdict\"]\n        reasoning = data[\"Reasoning\"]\n\n        try:\n            verdict = Verdict(verdict_str)\n        except ValueError:\n            raise ValueError(f\"Invalid verdict: {verdict_str}\")\n\n        return Score(score=score, verdict=verdict, reasoning=reasoning)\n\n    except (json.JSONDecodeError, ValueError, KeyError) as e:\n        raise ValueError(f\"Invalid score response format: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\n\ndef google_sheet_to_dataframe_api_key(spreadsheet_url, api_key):\n    \"\"\"\n    Reads a Google Spreadsheet using an API key and converts it to a pandas DataFrame\n    \n    Parameters:\n    spreadsheet_url (str): The URL of the Google Spreadsheet\n    api_key (str): Your Google API key\n    \n    Returns:\n    pandas.DataFrame: The spreadsheet data as a DataFrame\n    \"\"\"\n    try:\n        # Extract the spreadsheet ID from the URL\n        if '/d/' in spreadsheet_url:\n            spreadsheet_id = spreadsheet_url.split('/d/')[1].split('/')[0]\n        else:\n            raise ValueError(\"Invalid Google Spreadsheet URL format\")\n        \n        # Using the Google Sheets API v4\n        url = f\"https://sheets.googleapis.com/v4/spreadsheets/{spreadsheet_id}/values/Sheet1?key={api_key}\"\n        \n        # Make the API request\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Parse the response\n        data = response.json()\n        \n        if 'values' not in data:\n            raise ValueError(\"No values found in the spreadsheet\")\n            \n        # Extract header and data\n        headers = data['values'][0]\n        rows = data['values'][1:]\n        \n        # Create a list of dictionaries for each row\n        processed_data = []\n        for row in rows:\n            # Pad the row with None values if it's shorter than headers\n            padded_row = row + [None] * (len(headers) - len(row))\n            row_dict = dict(zip(headers, padded_row))\n            processed_data.append(row_dict)\n            \n        # Convert to DataFrame\n        df = pd.DataFrame(processed_data)\n        \n        return df\n    \n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.tools import tool\nfrom typing import Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langgraph.prebuilt import ToolNode\nimport csv\n\nmodel_used = \"gemini-2.0-flash\"\n\n# 🔧 Define the LLM used for scoring (returns structured output based on the Score schema)\nllm_scorer = ChatGoogleGenerativeAI(\n    model=model_used,\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",  # Expect JSON output\n        response_schema=Score,                  # Enforce structured return\n    )\n)\n\n# 🧠 LLM used by the chatbot node — this one is tool-aware via `bind_tools`\nllm = ChatGoogleGenerativeAI(model=model_used)\n\n\n@tool\ndef score_application(application: str) -> str:\n    \"\"\"\n    Function: score_application\n    Description:\n        This tool is called by the LLM to evaluate a membership application.\n        The input should be a text response covering motivation, background, and contribution intent.\n        The tool will generate a structured evaluation using the `Score` schema.\n\n    Parameters:\n        application (str): A full application text (answers to 2–3 onboarding questions).\n\n    Returns:\n        str: A structured JSON string containing the score, verdict, and reasoning.\n    \"\"\"\n    pass  # Placeholder, implemented by a separate node using llm_scorer\n\n\n@tool\ndef get_info(query: str) -> str:\n    \"\"\"\n    Function: get_info\n    Description:\n        This is a RAG-based tool that retrieves authoritative information from the community's knowledge base.\n        It performs semantic search over embedded documents using ChromaDB and returns the most relevant chunk(s)\n        with metadata like file source and confidence.\n\n    Parameters:\n        query (str): A user question or topic (e.g., \"What are the community values?\").\n\n    Returns:\n        str: A formatted string of the top matching chunk with confidence and source info.\n    \"\"\"\n    if collection:\n        embed_fn.document_mode = False  # Switch to query mode\n\n        result = collection.query(query_texts=[query], n_results=1)\n\n        documents = result.get(\"documents\", [[]])[0]\n        metadatas = result.get(\"metadatas\", [[]])[0]\n        distances = result.get(\"distances\", [[]])[0]\n\n        if not documents:\n            return \"Sorry, I couldn't find any relevant information in the knowledge base.\"\n\n        formatted_chunks = []\n\n        for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):\n            confidence = round(1 - dist, 2)\n            filename = meta.get(\"filename\", \"Unknown Source\")\n            page_number = meta.get(\"page_number\", \"Unknown page number\")\n            if int(page_number) == -1:\n                page_number = \"Unknown\"\n\n            chunk = (\n                f\"**Source:** {filename}, at Page {page_number}\\n\\n\"\n                f\"**Confidence:** {confidence}\\n\\n\"\n                f\"{doc}\"\n            )\n            formatted_chunks.append(chunk)\n\n        # Optional: Display summary in notebook output\n        #display(Markdown(\n        #    f\"📘 **Retrieved Information:**\\n\\n\"\n        #    f\"**Source:** {filename}, at Page {page_number}\\n\\n\"\n        #    f\"**Confidence:** {confidence}\\n\\n\"\n        #))\n\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n\n\n# Memory of previously scored application timestamps\nalready_scored_ids = set()\n\n@tool\ndef check_new_applications() -> str:\n    \"\"\"\n    Checks a Google Sheet for new community membership applications,\n    scores the new ones using the scoring agent, and returns a summary.\n\n    Parameters:\n        non needed, hard coded for now\n\n    Returns:\n        str: Markdown-formatted summary of the scores\n    \"\"\"\n    spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1vUs4m_hAaiI04iIWcKDQ7mV1j5LBwJUfxqviB1I6gCg/edit?usp=sharing\"\n    df = google_sheet_to_dataframe_api_key(spreadsheet_url, GOOGLE_SHEET_API_KEY)\n\n    if df is None or df.empty:\n        return \"⚠️ Could not load the spreadsheet or it's empty.\"\n\n    markdown_rows = []\n    csv_rows = []\n    new_found = False\n\n    for i, row in df.iterrows():\n        app_id = row.get(\"Timestamp\")\n        if app_id in already_scored_ids:\n            continue\n\n        new_found = True\n\n        try:\n            full_app = (\n                f\"1. Why do you want to join?   {row.get('Why do you want to join this community?')}\\n\"\n                f\"2. Your current interests or goals?   {row.get('What topics or goals are you most interested in right now?')}\\n\"\n                f\"3. Skills or experience?   {row.get('Do you have any experience or skills you’re excited to share with others?')}\\n\"\n                f\"4. Preferred participation style?   {row.get('How would you like to participate in the community?')}\\n\"\n                f\"5. Support or mentorship needs?   {row.get('What kind of support or mentorship are you looking for?')}\"\n            )\n\n            messages = [AISCORER_SYSINT, HumanMessage(content=full_app)]\n            result = llm_scorer.invoke(messages).content\n            parsed = parse_score_response(result)\n\n            already_scored_ids.add(app_id)\n\n            # Verdict emojis\n            emoji = {\n                \"approve\": \"✅\",\n                \"review\": \"⚠️\",\n                \"reject\": \"❌\"\n            }.get(parsed['verdict'].value, \"\")\n\n            reasoning_short = parsed['reasoning'].replace(\"\\n\", \" \").replace(\"|\", \"-\")[:100] + \"...\"\n\n            # Markdown row\n            markdown_rows.append(\n                f\"| {row.get('First Name', '')} {row.get('Last Name', '')} \"\n                f\"| {parsed['score']} \"\n                f\"| {emoji} {parsed['verdict'].value} \"\n                f\"| {reasoning_short} |\"\n            )\n\n            # CSV row\n            csv_rows.append({\n                \"Timestamp\": app_id,\n                \"Name\": f\"{row.get('First Name')} {row.get('Last Name')}\",\n                \"Score\": parsed[\"score\"],\n                \"Verdict\": parsed[\"verdict\"].value,\n                \"Reasoning\": parsed[\"reasoning\"]\n            })\n\n        except Exception as e:\n            markdown_rows.append(\n                f\"| Row {i+1} | ❌ | Error | {str(e)} |\"\n            )\n\n    if not new_found:\n        return \"✅ No new applications found.\"\n\n    # Save CSV\n    with open(\"scored_applications.csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=[\"Timestamp\", \"Name\", \"Score\", \"Verdict\", \"Reasoning\"])\n        writer.writeheader()\n        writer.writerows(csv_rows)\n\n    # Print Markdown Table\n    markdown_table = (\n        \"### ✅ Scored Applications\\n\\n\"\n        \"| Applicant | Score | Verdict | Reasoning |\\n\"\n        \"|-----------|-------|---------|-----------|\\n\"\n        + \"\\n\".join(markdown_rows)\n        + \"\\n\\n📄 CSV saved as `scored_applications.csv`\"\n    )\n\n    #display(Markdown(markdown_table))\n    return markdown_table\n\n\n\n# 🛠️ Register tools with LangGraph\n# Auto-tools will be called automatically by ToolNode (e.g., get_info)\nauto_tools = [get_info,check_new_applications]\n#auto_tools = [get_info]\ntool_node = ToolNode(auto_tools)\n\n# Manual tools are handled via custom logic (e.g., score_application routed to scoring node)\nother_tools = [score_application]\n\n# 🤖 Bind all tools to the chatbot LLM so it can call them when needed\nllm_with_tools = llm.bind_tools(auto_tools + other_tools)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nodes  ","metadata":{}},{"cell_type":"markdown","source":"🧩 **LangGraph Nodes: Chatbot, Human, and Scoring Logic**\n\nThis section defines the core **nodes** used by the LangGraph agent, each responsible for a different part of the conversation flow.\n\n- **`chatbot` node**: Handles general conversation and decides when to call tools. It uses a Gemini model (`llm_with_tools`) and follows a system prompt that defines its behavior and responsibilities.\n  \n- **`score_node`**: Executes when the model invokes the `score_application` tool. It sends the application text to a specialized scorer model, parses the structured output, and adds it to the state.\n\n- **`human_node`**: Manages the notebook interaction loop. It displays the model's message, accepts user input, and determines if the conversation should end.\n\n\nEach node contributes to enabling an intelligent, tool-using agent that can guide users, retrieve information, and evaluate applications in a human-like, structured workflow.\n","metadata":{}},{"cell_type":"code","source":"from langchain_core.messages.ai import AIMessage\nfrom langchain_core.messages.tool import ToolMessage\nfrom langchain_core.messages import HumanMessage\nimport ipywidgets as widgets\n\n\n\n\ndef score_node(state: ReviewState) -> ReviewState:\n    \"\"\"\n    Function: score_node\n    Description:\n        This node is triggered when the `score_application` tool is called by the LLM.\n\n        It extracts the application text from the tool call, sends it to the `llm_scorer` model with a\n        system instruction, and parses the structured response using `parse_score_response`.\n\n        The score and explanation are printed in the notebook, and the structured result is returned\n        in the updated state.\n\n    Returns:\n        ReviewState: Updated state containing the score, tool message, and application content.\n    \"\"\"\n    tool_msg = state.get(\"messages\", [])[-1]\n    outbound_msgs = []\n    parsed_score = None\n    application_text = None\n\n    for tool_call in tool_msg.tool_calls:\n        if tool_call['name'] == 'score_application':\n            application_text = tool_call[\"args\"][\"application\"]\n            messages = [AISCORER_SYSINT, HumanMessage(content=application_text)]\n            try:\n                response = llm_scorer.invoke(messages)\n                response_text = response.content\n\n                parsed_score = parse_score_response(response_text)\n\n                # Create a summary to show in chat (optional)\n                summary_msg = AIMessage(content=(\n                    f\"### ✅ Application Scored\\n\"\n                    f\"- **Score**: `{parsed_score['score']}`\\n\"\n                    f\"- **Verdict**: `{parsed_score['verdict'].value}`\\n\"\n                    f\"- **Reasoning**:\\n\\n{parsed_score['reasoning']}\"\n                ))\n\n                outbound_msgs.append(\n                    ToolMessage(\n                        content=response_text,\n                        name=tool_call[\"name\"],\n                        tool_call_id=tool_call[\"id\"],\n                    )\n                )\n            except Exception as e:\n                error_msg = AIMessage(content=f\"❌ Scoring failed: {e}\")\n                outbound_msgs.append(error_msg)\n                raise\n        else:\n            raise NotImplementedError(f\"Unknown tool call: {tool_call['name']}\")\n\n    return {\n        \"messages\": outbound_msgs,\n        \"applications\": [application_text],\n        \"scores\": [parsed_score] if parsed_score else [],\n    }\n\n\ndef chatbot(state: ReviewState) -> ReviewState:\n    \"\"\"\n    Function: chatbot\n    Description:\n        Core chatbot logic for continuing the conversation. It routes the conversation to\n        the Gemini model with access to system instructions and prior messages.\n\n        If there are no previous messages, it starts the conversation with a welcome message.\n\n    Returns:\n        ReviewState: Updated state with the assistant's response appended to messages.\n    \"\"\"\n    if state[\"messages\"]:\n        #new_output = llm_with_tools.invoke([AIONBOARD_SYSINT] + state[\"messages\"])\n        # Remove any SystemMessage if present (Gemini doesn't like it here)\n        #messages = [m for m in state[\"messages\"] if not isinstance(m, SystemMessage)]\n        messages = [m for m in state[\"messages\"]]\n        new_output = llm_with_tools.with_config({\n            \"system_instruction\": AIONBOARD_SYSINT\n        }).invoke(messages)\n    else:\n        new_output = AIMessage(content='Hi I am your community friendly AI companion ! How can I help?')\n\n    return state | {\"messages\": [new_output]}\n\n\ndef human_node(state: ReviewState, user_input: str = None) -> ReviewState:\n    \"\"\"\n    Gradio-compatible human input node.\n    Adds the user's message to the state if provided.\n    \"\"\"\n    if state.get(\"messages\"):\n        last_msg = state[\"messages\"][-1]\n        if isinstance(last_msg, AIMessage):\n            display(Markdown(f\"**🤖 Model:**\\n\\n{last_msg.content}\"))\n\n    if not user_input:\n        print(\"⚠️ No user input provided.\")\n        return state\n\n    if user_input.lower().strip() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n        state[\"conversation_done\"] = True\n\n    return state | {\"messages\": [HumanMessage(content=user_input)]}\n\n\ndef human_node_backup(state: ReviewState) -> ReviewState:\n    \"\"\"\n    Function: human_node\n    Description:\n        This node handles the human interaction loop in a notebook environment.\n\n        It displays the most recent AI message and prompts the user for input. If the user\n        types an exit keyword, the conversation is flagged as completed.\n\n    Returns:\n        ReviewState: Updated state with the user's message added.\n    \"\"\"\n    last_msg = state[\"messages\"][-1]\n\n    if isinstance(last_msg, AIMessage):\n        display(Markdown(f\"**🤖 Model:**\\n\\n{last_msg.content}\"))\n    else:\n        print(\"🤖 Model:\", last_msg)\n\n    user_input = input(\"👤 You: \")\n\n    if user_input.lower().strip() in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n        state[\"conversation_done\"] = True\n\n    return state | {\"messages\": [HumanMessage(content=user_input)]}\n  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Routing ","metadata":{}},{"cell_type":"markdown","source":"🧭 **Conditional Routing Logic**\n\nThis section defines the routing functions that LangGraph uses to determine the next step in the conversation based on the current state.\n\n- **`maybe_exit_human_node`**: Checks if the user wants to quit (e.g., typing \"q\" or \"exit\"). If so, it ends the conversation. Otherwise, it continues chatting.\n\n- **`maybe_route_to_tools`**: After the LLM responds, this function decides what should happen next:\n  - If the model made a valid tool call → route to `tools` or `scoring`.\n  - If there's no tool call → go to `human` for more input.\n  - If the user ended the chat → go to `__end__`.\n\nThese routes act as decision points that dynamically guide the agent's workflow during each turn of the conversation.\n","metadata":{}},{"cell_type":"code","source":"def maybe_exit_human_node(state: ReviewState) -> Literal[\"chatbot\", \"__end__\"]:\n    \"\"\"\n    Function: maybe_exit_human_node\n    Description:\n        Determines whether the conversation should continue or end after user input.\n\n        If the `conversation_done` flag is True (e.g., user typed \"quit\"), the graph returns __end__.\n        Otherwise, it loops back to the chatbot node to continue the conversation.\n\n    Returns:\n        Literal[\"chatbot\", \"__end__\"]: The next node to route to.\n    \"\"\"\n    if state.get(\"conversation_done\", False):\n        return END\n    else:\n        return \"chatbot\"\n\n\ndef maybe_route_to_tools(state: ReviewState) -> Literal[\"chatbot\", \"human\",\"tools\", \"scoring\", \"__end__\"]:\n    \"\"\"\n    Function: maybe_route_to_tools\n    Description:\n        Controls how the conversation flows after a model response.\n\n        Based on the most recent message, it checks:\n        - If the conversation is done → end it.\n        - If the model made a tool call → route to `tools` or `scoring`.\n        - Otherwise → hand off to `human` for more input.\n\n    Returns:\n        Literal: The name of the next node to activate.\n    \"\"\"\n    if not (msgs := state.get(\"messages\", [])):\n        raise ValueError(f\"No messages found when parsing state: {state}\")\n\n    msg = msgs[-1]\n\n    if state.get(\"conversation_done\", False):\n        return END\n\n    elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n        # Check if tool call is known and registered in the tool node\n        if any(\n            tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls\n        ):\n            return \"tools\"\n        else:\n            return \"scoring\"\n\n    else:\n        return \"human\" \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Graph Builder","metadata":{}},{"cell_type":"markdown","source":"🧠 **Building the Agent Graph with LangGraph**\n\nThis section constructs the full agent workflow as a graph using the `StateGraph` interface from LangGraph.\n\n- Each **node** in the graph represents a step in the conversation flow, such as `chatbot`, `human`, `tools`, or `scoring`.\n- Conditional routing logic is added to allow the graph to dynamically transition between nodes based on the conversation state and tool calls.\n- After calling a tool or completing a scoring task, the graph routes the result back to the `chatbot` to continue the dialogue.\n- The chatbot is set as the starting point of the graph using the `START` edge.\n\nThis modular architecture enables structured, multi-turn conversations where the LLM can delegate tasks (like scoring or retrieving knowledge) and loop back naturally.\n","metadata":{}},{"cell_type":"code","source":"# 🧱 Initialize a stateful LangGraph based on the ReviewState schema\ngraph_builder = StateGraph(ReviewState)\n\n# 🔹 Add functional nodes to the graph\ngraph_builder.add_node(\"chatbot\", chatbot)       # LLM node that handles conversation\ngraph_builder.add_node(\"human\", human_node)      # Node for interactive user input (notebook)\ngraph_builder.add_node(\"scoring\", score_node)    # Node for application evaluation\ngraph_builder.add_node(\"tools\", tool_node)       # Node for auto-invoked tools (e.g., get_info)\n\n# 🔁 Add conditional transitions based on output from nodes\ngraph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)      # Decide where to go after the model replies\n#graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)       # Decide whether to quit or keep chatting\n\n# 🔄 Route tool responses back to the chatbot\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(\"scoring\", \"chatbot\")\n\n# 🚀 Define entrypoint: the chatbot is the first node to run\ngraph_builder.add_edge(START, \"chatbot\")\n\n# ✅ Compile the graph into a runnable LangGraph instance\nchat_graph = graph_builder.compile()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🗺️ **Agent Graph Visualization**\n\nThe diagram below visualizes the full LangGraph execution flow used in this AI agent.\n\n- Each box represents a **node** (e.g., chatbot, human, scoring).\n- Arrows indicate the **flow of state and messages** between nodes.\n- Conditional logic (e.g., routing based on tool calls) determines which path is followed at runtime.\n\nThis graph structure enables flexible, multi-step decision-making and clean integration of tools like RAG and scoring.\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display, Markdown\n\n# 🔍 Visualize the compiled LangGraph as a Mermaid diagram (rendered as PNG)\n#Image(chat_graph.get_graph().draw_mermaid_png())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🧪 Launching the Agent\n\nWe now invoke the compiled LangGraph agent with an initial empty state.\n\n- `messages: []` means this is the start of a new conversation.\n- The `recursion_limit` ensures the graph doesn't run forever (e.g., in case of unexpected loops).\n- The graph begins at the `chatbot` node, which sends a welcome message and waits for user input.\n\nThis step effectively \"boots up\" the AI onboarding assistant and prepares it to handle user interactions in a dynamic, tool-augmented way.\n","metadata":{}},{"cell_type":"code","source":"# ⚙️ Optional: Set graph execution limits to prevent infinite loops\n#config = {\"recursion_limit\": 30}\n\n# 🚀 Start the agent graph with an empty message history\n# This triggers the chatbot node, which welcomes the user and begins the interaction\n#state = chat_graph.invoke({\"messages\": []}, config)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Application example for Testing","metadata":{}},{"cell_type":"code","source":"\n\napplication = \"\"\"1. Why do you want to join?  \nI want to join to connect with like-minded professionals in tech, share knowledge, and contribute to a vibrant and supportive community. I'm also looking for opportunities to mentor others and grow my network.\n\n2. Your background/profession?  \nI'm a senior software engineer with 6 years of experience in backend development, currently working on cloud infrastructure and AI integrations.\n\n3. How would you like to contribute?  \nI can help with organizing technical talks, mentorship programs, and contribute to online discussions. I also enjoy helping with community outreach and creating content.\n\n4. What areas are you most interested in?  \nAI/ML, cloud technologies, community building, and career growth.\n\n5. How much time can you dedicate monthly?  \n5-8 hours/month.\n\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chat UI","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\nconversation_state = {\"messages\": []}\n\n# Action log to track backend events\naction_log = []\n\ndef check_applications_via_button(history):\n    simulated_message = \"<tool_call>check_new_applications()</tool_call>\"\n    response, log = agent_response(simulated_message, history)\n    return [['assistant',response]],log\n\n\ndef agent_response(message, history):\n    # Append the new message to the conversation state\n    action_log.append(f\"🧑 User: {message[:100]}...\")\n    conversation_state[\"messages\"].append(HumanMessage(content=message))\n\n    try:\n        # Run through the LangGraph agent\n        updated_state = chat_graph.invoke(conversation_state)\n        conversation_state.update(updated_state)\n\n        # Get last assistant message\n        last = conversation_state[\"messages\"][-1]\n        reply = last.content if hasattr(last, \"content\") else \"\"\n        if len(conversation_state[\"messages\"]) >=2:\n            before_last=conversation_state[\"messages\"][-2]\n        if hasattr(before_last, \"tool_call_id\"):\n            action_log.append('🛠️ Tool Call → '+ before_last.name)\n        else:\n            action_log.append(\"🤖 Assistant responded.\")\n        return reply, \"\\n\".join(action_log)\n    except Exception as e:\n        return f\"❌ Error: {str(e)}\"\n\n\n\n\nlog_output = gr.Textbox(label=\"📝 Agent Log\", lines=6, interactive=False)\n\nchatbot = gr.ChatInterface(\n    fn=agent_response,\n    additional_outputs=[log_output],\n    title=\"\",\n    submit_btn=\"Send\"\n)\n\nwith gr.Blocks() as ui:\n    gr.Markdown(\"## 🤖 Community Onboarding Agent\")\n    chatbot.render()\n    log_output.render()\n    # Check New Applications button — needs access to chatbot.history\n    gr.Button(\"📋 Check New Applications\").click(\n        fn=check_applications_via_button,\n        inputs=[chatbot.chatbot],   # this provides history\n        outputs=[chatbot.chatbot,log_output]\n    )\n\n\n\nui.launch()\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(chatbot.chatbot)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n\n\n# --- Check applications button handler ---\ndef check_applications(history, agent_log):\n    simulated_input = \"<tool_call>check_new_applications()</tool_call>\"\n    return agent_response(simulated_input, history, agent_log)\n\n# --- Gradio ChatInterface ---\nchat = gr.ChatInterface(\n    fn=agent_response,\n    additional_inputs=[gr.Textbox(label=\"🔍 Agent Log\", lines=6, interactive=False)],\n    textbox=gr.Textbox(placeholder=\"Ask your question here...\"),\n    title=\"🤖 Community Onboarding Agent\",\n    description=\"Ask me anything about the community or say `check new applications` to score them.\",\n)\n\n# --- Add extra button ---\nwith chat:\n    check_button = gr.Button(\"📋 Check New Applications\")\n    check_button.click(\n        fn=check_applications,\n        inputs=[chat.chatbot, chat.additional_inputs[0]],\n        outputs=[chat.chatbot, chat.additional_inputs[0]]\n    )\n\nchat.launch()\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}